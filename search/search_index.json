{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBM Watson Libraries for Embed \u00b6 This repo contains resources and examples to get started with IBM Watson Libraries for Embed.","title":"Home"},{"location":"#ibm-watson-libraries-for-embed","text":"This repo contains resources and examples to get started with IBM Watson Libraries for Embed.","title":"IBM Watson Libraries for Embed"},{"location":"nlp-combined-image/","text":"Running NLP models using a single container image \u00b6 This page describes how to run NLP models by combining the runtime and models into a single container The product documentation will provide more details and additional capabilities not shown in this example. Access to images \u00b6 To run this example you need to have access to the IBM Watson Libraries for Embed container images. These are available on the IBM Container Registry, but you need an entitlement key For the code in this repo I use a local mirror, hosted in a Project Quay repository. If you are running against the IBM registry then you will need to change the images from lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs to cp.icr.io/cp/ai and login to the repository before trying to access any images with Docker Podman docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key> Building the runtime \u00b6 To create a usable NLP runtime container, the standard runtime needs to be combined with appropriate model(s). There are a number of standard models provided. The files used in this example can be found here . To build the model directory run the script: Docker Podman docker build -f Containerfile . -t nlp-standalone:latest podman build --tls-verify = false -f Containerfile . -t nlp-standalone:latest To build the container image run the command: Docker Podman docker build -f Containerfile . -t nlp-standalone podman build --tls-verify = false -f Containerfile . -t nlp-standalone You should store the container image in a repository if you want to use it on a Kubernetes platform or make it available to other developers to run locally. If you want to run the container on a serverless platform, such as IBM Code Engine or AWS Fargate then the registry needs to be accessible from those platforms. Docker Podman docker tag nlp-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 docker push lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 podman tag nlp-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 podman push --tls-verify = false lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 Running the container \u00b6 The container can be run on your local system. The following commands assume you have the tool on your local system (either built or pulled from a repository) Docker Podman docker run --rm -it -e ACCEPT_LICENSE = true -e LOCAL_MODELS_DIR = /app/model_data -p 8085 :8085 -p 8080 :8080 nlp-standalone podman run --rm -it -e ACCEPT_LICENSE = true -e LOCAL_MODELS_DIR = /app/model_data -p 8085 :8085 -p 8080 :8080 nlp-standalone Submitting requests to the container \u00b6 curl -s \\ \"http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -d '{ \"raw_document\": { \"text\": \"This is a test sentence\" }, \"parsers\": [\"token\"] }' Running the container on IBM Code Engine \u00b6 This assumes you have an account on IBM cloud, have an instance of Container Registry in your account, have the ibmcloud CLI installed and the Container Registry and Code Engine plugins installed A full tutorial can be found here login to the IBM Cloud on the command line ibmcloud login (--sso) set the region for the Container Registry service on IBM Cloud ibmcloud cr region-set then select the appropriate region and note the name of the registry (e.g. uk-south uses registry uk.icr.io ) login to the Container Registry ibmcloud cr login --client podman tag the stand alone image for the IBM Cloud registry (substitute for your region registry address) podman tag nlp-standalone:latest uk.icr.io/bi-uk/nlp-standalone:latest push the image to the registry podman push uk.icr.io/bi-uk/nlp-standalone:latest set the correct resource group to run the application in ibmcloud target -g bi-devops create a container engine project ibmcloud ce project create --name bi-nlp-standalone set the new project as the current context ibmcloud ce project select --name bi-nlp-standalone use the IBM Cloud web UI to deploy the code engine app, specifying the image in the IBM Container Registry you pushed in step 5. create the environment variable ACCEPT_LICENSE with the value true Once the Code Engine application is deployed you can verify it is running with ibmcloud ce app list Test the application is responding to requests using curl ( you can get your application endpoint from the Domain mappings tab in the web UI ) curl -s -X POST \"https://nlp-standalone-application-4a.wgry41hvzqj.eu-gb.codeengine.appdomain.cloud/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -H \"content-type: application/json\" \\ -d \"{ \\\"rawDocument\\\": { \\\"text\\\": \\\"This is a test.\\\" }, \\\"parsers\\\": [ \\\"TOKEN\\\" ]}\" \\ | jq -r .","title":"Stand alone container"},{"location":"nlp-combined-image/#running-nlp-models-using-a-single-container-image","text":"This page describes how to run NLP models by combining the runtime and models into a single container The product documentation will provide more details and additional capabilities not shown in this example.","title":"Running NLP models using a single container image"},{"location":"nlp-combined-image/#access-to-images","text":"To run this example you need to have access to the IBM Watson Libraries for Embed container images. These are available on the IBM Container Registry, but you need an entitlement key For the code in this repo I use a local mirror, hosted in a Project Quay repository. If you are running against the IBM registry then you will need to change the images from lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs to cp.icr.io/cp/ai and login to the repository before trying to access any images with Docker Podman docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key>","title":"Access to images"},{"location":"nlp-combined-image/#building-the-runtime","text":"To create a usable NLP runtime container, the standard runtime needs to be combined with appropriate model(s). There are a number of standard models provided. The files used in this example can be found here . To build the model directory run the script: Docker Podman docker build -f Containerfile . -t nlp-standalone:latest podman build --tls-verify = false -f Containerfile . -t nlp-standalone:latest To build the container image run the command: Docker Podman docker build -f Containerfile . -t nlp-standalone podman build --tls-verify = false -f Containerfile . -t nlp-standalone You should store the container image in a repository if you want to use it on a Kubernetes platform or make it available to other developers to run locally. If you want to run the container on a serverless platform, such as IBM Code Engine or AWS Fargate then the registry needs to be accessible from those platforms. Docker Podman docker tag nlp-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 docker push lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 podman tag nlp-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1 podman push --tls-verify = false lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/nlp-standalone:0.0.1","title":"Building the runtime"},{"location":"nlp-combined-image/#running-the-container","text":"The container can be run on your local system. The following commands assume you have the tool on your local system (either built or pulled from a repository) Docker Podman docker run --rm -it -e ACCEPT_LICENSE = true -e LOCAL_MODELS_DIR = /app/model_data -p 8085 :8085 -p 8080 :8080 nlp-standalone podman run --rm -it -e ACCEPT_LICENSE = true -e LOCAL_MODELS_DIR = /app/model_data -p 8085 :8085 -p 8080 :8080 nlp-standalone","title":"Running the container"},{"location":"nlp-combined-image/#submitting-requests-to-the-container","text":"curl -s \\ \"http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -d '{ \"raw_document\": { \"text\": \"This is a test sentence\" }, \"parsers\": [\"token\"] }'","title":"Submitting requests to the container"},{"location":"nlp-combined-image/#running-the-container-on-ibm-code-engine","text":"This assumes you have an account on IBM cloud, have an instance of Container Registry in your account, have the ibmcloud CLI installed and the Container Registry and Code Engine plugins installed A full tutorial can be found here login to the IBM Cloud on the command line ibmcloud login (--sso) set the region for the Container Registry service on IBM Cloud ibmcloud cr region-set then select the appropriate region and note the name of the registry (e.g. uk-south uses registry uk.icr.io ) login to the Container Registry ibmcloud cr login --client podman tag the stand alone image for the IBM Cloud registry (substitute for your region registry address) podman tag nlp-standalone:latest uk.icr.io/bi-uk/nlp-standalone:latest push the image to the registry podman push uk.icr.io/bi-uk/nlp-standalone:latest set the correct resource group to run the application in ibmcloud target -g bi-devops create a container engine project ibmcloud ce project create --name bi-nlp-standalone set the new project as the current context ibmcloud ce project select --name bi-nlp-standalone use the IBM Cloud web UI to deploy the code engine app, specifying the image in the IBM Container Registry you pushed in step 5. create the environment variable ACCEPT_LICENSE with the value true Once the Code Engine application is deployed you can verify it is running with ibmcloud ce app list Test the application is responding to requests using curl ( you can get your application endpoint from the Domain mappings tab in the web UI ) curl -s -X POST \"https://nlp-standalone-application-4a.wgry41hvzqj.eu-gb.codeengine.appdomain.cloud/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -H \"content-type: application/json\" \\ -d \"{ \\\"rawDocument\\\": { \\\"text\\\": \\\"This is a test.\\\" }, \\\"parsers\\\": [ \\\"TOKEN\\\" ]}\" \\ | jq -r .","title":"Running the container on IBM Code Engine"},{"location":"nlp-custom-models/","text":"Natural Language Processing Custom models \u00b6 Natural Language Processing (NLP) provides a set of libraries within Watson Studio to allow Data Scientists to create bespoke models. Theses can be exported from Watson Studio as an compressed archive by adding the following to a cell at the end of the model project . save_data ( 'ensemble_model' , data = ensemble_model . as_file_like_object (), overwrite = True ) this can then be transferred from the Watson Studio environment to your workstation (copy the file into a models directory). You can create a combined container with the runtime and model ARG WATSON_RUNTIME_BASE = \"cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18\" FROM ${WATSON_RUNTIME_BASE} as base ENV LOCAL_MODELS_DIR = /app/models COPY models /app/models Once built the container can now be used as outlined in the combined model section You can also use the packaging tool to create the containers","title":"custom model"},{"location":"nlp-custom-models/#natural-language-processing-custom-models","text":"Natural Language Processing (NLP) provides a set of libraries within Watson Studio to allow Data Scientists to create bespoke models. Theses can be exported from Watson Studio as an compressed archive by adding the following to a cell at the end of the model project . save_data ( 'ensemble_model' , data = ensemble_model . as_file_like_object (), overwrite = True ) this can then be transferred from the Watson Studio environment to your workstation (copy the file into a models directory). You can create a combined container with the runtime and model ARG WATSON_RUNTIME_BASE = \"cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18\" FROM ${WATSON_RUNTIME_BASE} as base ENV LOCAL_MODELS_DIR = /app/models COPY models /app/models Once built the container can now be used as outlined in the combined model section You can also use the packaging tool to create the containers","title":"Natural Language Processing Custom models"},{"location":"nlp-external-volume/","text":"Natural Language Processing - external volume \u00b6 This page covers how to setup Natural Language Processing (NLP) using the run time with the models contained on an external volume which is mapped into the runtime The files relating to this example can be found in the repository here Rather than combining the model and runtime into a single container it is also possible to create an external volume, copy the models onto that volume, then map the volume into the runtime container when it is run. The model containers have their default entrypoint set to run /bin/sh -c /app/unpack_model.sh which will unpack the model within the container to the directory pointed at by the MODEL_ROOT_DIR directory, defaults to /app/modesl Simple running all of the required model containers with the external volume mapped to /app/models will create the external volume, which can then be mapped into the runtime. Again, the environment variable MODEL_ROOT_DIR tells the runtime where to find available models. There are 2 methods of deploying the models shown. One using a local environment with docker or podman and one using init containers in an OpenShift deployment. Local deploy \u00b6 To use docker or podman on your local system you can modify then run the following script as required. These files are available in the git repo as podman-local.sh or docker-local.sh: #!/usr/bin/env bash IMAGE_REGISTRY = ${ IMAGE_REGISTRY :- \"lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs\" } RUNTIME_IMAGE = ${ RUNTIME_IMAGE :- \"watson-nlp-runtime:1.0.20\" } export MODELS = \" ${ MODELS :- \"watson-nlp_syntax_izumo_lang_en_stock:1.0.7,watson-nlp_syntax_izumo_lang_fr_stock:1.0.7\" } \" IFS = ',' read -ra models_arr <<< \" ${ MODELS } \" TLS_CERT = ${ TLS_CERT :- \"\" } TLS_KEY = ${ TLS_KEY :- \"\" } CA_CERT = ${ CA_CERT :- \"\" } function real_path { echo \" $( cd $( dirname ${ 1 } ) && pwd ) / $( basename ${ 1 } ) \" } # Clear out existing volume podman volume rm model_data 2 >/dev/null || true # Create a shared volume and initialize with open permissions podman volume create --label model_data podman run --rm -it -v model_data:/model_data alpine chmod 777 /model_data # Put models into the shared volume for model in \" ${ models_arr [@] } \" do podman run --rm -it --tls-verify = false -v model_data:/app/models -e ACCEPT_LICENSE = true $IMAGE_REGISTRY / $model done # If TLS credentials are set up, run with TLS tls_args = \"\" if [ \" $TLS_CERT \" ! = \"\" ] && [ \" $TLS_KEY \" ! = \"\" ] then echo \"Running with TLS\" tls_args = \" $tls_args -v $( real_path ${ TLS_KEY } ) :/tls/server.key.pem\" tls_args = \" $tls_args -e TLS_SERVER_KEY=/tls/server.key.pem\" tls_args = \" $tls_args -e SERVE_KEY=/tls/server.key.pem\" tls_args = \" $tls_args -v $( real_path ${ TLS_CERT } ) :/tls/server.cert.pem\" tls_args = \" $tls_args -e TLS_SERVER_CERT=/tls/server.cert.pem\" tls_args = \" $tls_args -e SERVE_CERT=/tls/server.cert.pem\" tls_args = \" $tls_args -e PROXY_CERT=/tls/server.cert.pem\" if [ \" $CA_CERT \" ! = \"\" ] then echo \"Enabling mTLS\" tls_args = \" $tls_args -v $( real_path ${ CA_CERT } ) :/tls/ca.cert.pem\" tls_args = \" $tls_args -e TLS_CLIENT_CERT=/tls/ca.cert.pem\" tls_args = \" $tls_args -e MTLS_CLIENT_CA=/tls/ca.cert.pem\" tls_args = \" $tls_args -e PROXY_MTLS_KEY=/tls/server.key.pem\" tls_args = \" $tls_args -e PROXY_MTLS_CERT=/tls/server.cert.pem\" fi echo \"TLS args: [ $tls_args ]\" fi # Run the runtime with the models mounted podman run ${ @ } \\ --rm -it --tls-verify = false \\ -v model_data:/app/model_data \\ -e ACCEPT_LICENSE = true \\ -e LOCAL_MODELS_DIR = /app/model_data \\ -p 8085 :8085 \\ -p 8080 :8080 \\ $tls_args $IMAGE_REGISTRY / $RUNTIME_IMAGE ``` ### Testing the model To test the running model you can use curl ``` shell curl -s \\ \"http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -d '{ \"raw_document\": { \"text\": \"This is a test sentence\" }, \"parsers\": [\"token\"] }' Deploy to OpenShift \u00b6 Navigate to where the files from this repo have been cloned to your local machine and change to the speech/tts_kube directory Create a new project on the cluster oc new-project demo-nlp Apply all the manifest files to your cluster oc apply -f <directory containing manifest files> Testing the deployment \u00b6 To test the nlp container the curl utility can be used to submit requests, alter the URL to the route of the service on your cluster: curl -k -s \"https://watson-nlp-container-demo-nlp.apps.ocp.lab.home/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -d '{ \"raw_document\": { \"text\": \"This is a test sentence\" }, \"parsers\": [\"token\"] }' Refer to the API reference for details of the requests that can be made","title":"External volume"},{"location":"nlp-external-volume/#natural-language-processing-external-volume","text":"This page covers how to setup Natural Language Processing (NLP) using the run time with the models contained on an external volume which is mapped into the runtime The files relating to this example can be found in the repository here Rather than combining the model and runtime into a single container it is also possible to create an external volume, copy the models onto that volume, then map the volume into the runtime container when it is run. The model containers have their default entrypoint set to run /bin/sh -c /app/unpack_model.sh which will unpack the model within the container to the directory pointed at by the MODEL_ROOT_DIR directory, defaults to /app/modesl Simple running all of the required model containers with the external volume mapped to /app/models will create the external volume, which can then be mapped into the runtime. Again, the environment variable MODEL_ROOT_DIR tells the runtime where to find available models. There are 2 methods of deploying the models shown. One using a local environment with docker or podman and one using init containers in an OpenShift deployment.","title":"Natural Language Processing - external volume"},{"location":"nlp-external-volume/#local-deploy","text":"To use docker or podman on your local system you can modify then run the following script as required. These files are available in the git repo as podman-local.sh or docker-local.sh: #!/usr/bin/env bash IMAGE_REGISTRY = ${ IMAGE_REGISTRY :- \"lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs\" } RUNTIME_IMAGE = ${ RUNTIME_IMAGE :- \"watson-nlp-runtime:1.0.20\" } export MODELS = \" ${ MODELS :- \"watson-nlp_syntax_izumo_lang_en_stock:1.0.7,watson-nlp_syntax_izumo_lang_fr_stock:1.0.7\" } \" IFS = ',' read -ra models_arr <<< \" ${ MODELS } \" TLS_CERT = ${ TLS_CERT :- \"\" } TLS_KEY = ${ TLS_KEY :- \"\" } CA_CERT = ${ CA_CERT :- \"\" } function real_path { echo \" $( cd $( dirname ${ 1 } ) && pwd ) / $( basename ${ 1 } ) \" } # Clear out existing volume podman volume rm model_data 2 >/dev/null || true # Create a shared volume and initialize with open permissions podman volume create --label model_data podman run --rm -it -v model_data:/model_data alpine chmod 777 /model_data # Put models into the shared volume for model in \" ${ models_arr [@] } \" do podman run --rm -it --tls-verify = false -v model_data:/app/models -e ACCEPT_LICENSE = true $IMAGE_REGISTRY / $model done # If TLS credentials are set up, run with TLS tls_args = \"\" if [ \" $TLS_CERT \" ! = \"\" ] && [ \" $TLS_KEY \" ! = \"\" ] then echo \"Running with TLS\" tls_args = \" $tls_args -v $( real_path ${ TLS_KEY } ) :/tls/server.key.pem\" tls_args = \" $tls_args -e TLS_SERVER_KEY=/tls/server.key.pem\" tls_args = \" $tls_args -e SERVE_KEY=/tls/server.key.pem\" tls_args = \" $tls_args -v $( real_path ${ TLS_CERT } ) :/tls/server.cert.pem\" tls_args = \" $tls_args -e TLS_SERVER_CERT=/tls/server.cert.pem\" tls_args = \" $tls_args -e SERVE_CERT=/tls/server.cert.pem\" tls_args = \" $tls_args -e PROXY_CERT=/tls/server.cert.pem\" if [ \" $CA_CERT \" ! = \"\" ] then echo \"Enabling mTLS\" tls_args = \" $tls_args -v $( real_path ${ CA_CERT } ) :/tls/ca.cert.pem\" tls_args = \" $tls_args -e TLS_CLIENT_CERT=/tls/ca.cert.pem\" tls_args = \" $tls_args -e MTLS_CLIENT_CA=/tls/ca.cert.pem\" tls_args = \" $tls_args -e PROXY_MTLS_KEY=/tls/server.key.pem\" tls_args = \" $tls_args -e PROXY_MTLS_CERT=/tls/server.cert.pem\" fi echo \"TLS args: [ $tls_args ]\" fi # Run the runtime with the models mounted podman run ${ @ } \\ --rm -it --tls-verify = false \\ -v model_data:/app/model_data \\ -e ACCEPT_LICENSE = true \\ -e LOCAL_MODELS_DIR = /app/model_data \\ -p 8085 :8085 \\ -p 8080 :8080 \\ $tls_args $IMAGE_REGISTRY / $RUNTIME_IMAGE ``` ### Testing the model To test the running model you can use curl ``` shell curl -s \\ \"http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -d '{ \"raw_document\": { \"text\": \"This is a test sentence\" }, \"parsers\": [\"token\"] }'","title":"Local deploy"},{"location":"nlp-external-volume/#deploy-to-openshift","text":"Navigate to where the files from this repo have been cloned to your local machine and change to the speech/tts_kube directory Create a new project on the cluster oc new-project demo-nlp Apply all the manifest files to your cluster oc apply -f <directory containing manifest files>","title":"Deploy to OpenShift"},{"location":"nlp-external-volume/#testing-the-deployment","text":"To test the nlp container the curl utility can be used to submit requests, alter the URL to the route of the service on your cluster: curl -k -s \"https://watson-nlp-container-demo-nlp.apps.ocp.lab.home/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock\" \\ -d '{ \"raw_document\": { \"text\": \"This is a test sentence\" }, \"parsers\": [\"token\"] }' Refer to the API reference for details of the requests that can be made","title":"Testing the deployment"},{"location":"nlp-modelmesh/","text":"Natural Language Processing using KServe Model Mesh \u00b6 The KServe project has the capability to serve models from an S3 bucket, without having to prepare each model and run them as separate applications on OpenShif or Kubernetes. Warning Currently the install instructions for KServe ModelMesh only work on a vanilla Kubernetes, they do not work on OpenShift as the instructions break the Security Context additional security measures deployed on OpenShift. To use a local deploy of KServer models mesh you can use minikube, alternatively a cloud hosted Kubernetes Service should work. install minikube and enable the dashboard, ingress, ingress-dns, metrics-server addons, ensuring you provide sufficient CPU and memory resources - models are resource hungry! deploy KServe ModelMesh using the quickstart instructions create the image pull secret kubectl create secret docker-registry ibm-entitlement-key --docker-server=cp.icr.io --docker-username=cp --docker-password=<entitlement key> --docker-email=<your-email> create the service account `kubectl apply -f service-account.yaml update the model serving config kubectl apply -f config.yaml create the watson runtim within kserve kubectl apply -f runtime.yaml upload the required model(s) to the minio S3 service kubectl apply -f upload.yaml create the inferencing service kubectl apply -f inferencing-service.yaml you can then use the provided client application, client.py, to test the service. You will need to install the python library pip install watson-nlp-runtime-client","title":"KServe modelmesh"},{"location":"nlp-modelmesh/#natural-language-processing-using-kserve-model-mesh","text":"The KServe project has the capability to serve models from an S3 bucket, without having to prepare each model and run them as separate applications on OpenShif or Kubernetes. Warning Currently the install instructions for KServe ModelMesh only work on a vanilla Kubernetes, they do not work on OpenShift as the instructions break the Security Context additional security measures deployed on OpenShift. To use a local deploy of KServer models mesh you can use minikube, alternatively a cloud hosted Kubernetes Service should work. install minikube and enable the dashboard, ingress, ingress-dns, metrics-server addons, ensuring you provide sufficient CPU and memory resources - models are resource hungry! deploy KServe ModelMesh using the quickstart instructions create the image pull secret kubectl create secret docker-registry ibm-entitlement-key --docker-server=cp.icr.io --docker-username=cp --docker-password=<entitlement key> --docker-email=<your-email> create the service account `kubectl apply -f service-account.yaml update the model serving config kubectl apply -f config.yaml create the watson runtim within kserve kubectl apply -f runtime.yaml upload the required model(s) to the minio S3 service kubectl apply -f upload.yaml create the inferencing service kubectl apply -f inferencing-service.yaml you can then use the provided client application, client.py, to test the service. You will need to install the python library pip install watson-nlp-runtime-client","title":"Natural Language Processing using KServe Model Mesh"},{"location":"nlp/","text":"Natural Language Processing \u00b6 There are a number of Natural Language Processing (NLP) services offered, covering a number of languages, within the IBM Watson Libraries for Embed. The model catalog lists all available models. In addition to the models there is a runtime which supports both gRPC and REST enpoints. Deployment options \u00b6 When deploying a model you need to combine the models you want with the container and there are a number of ways to do this: Create a single container with the runtime and model(s) within the container Combine the models in a single directory structure, then map that directory in to the runtime container The model containers available on the IBM Container Registry do not have a runtime installed. Their default entry point is /bin/sh -c /app/unpack_model.sh , which will expand the model into directory /app/models . This means running a model container will expand the model into directory /app/models, so if this is an external volume mounted into the container the model will be on the external volume, which could then be mounted into the NLP runtime container, which will make the model available to the runtime without having to be installed in the same container as the runtime. The NLP containers can get very large, with the runtime currently being 2.5GB> Startup times need to be considered when adding multiple models into a single container with the NLP runtime. Examples \u00b6 This project contains a number of examples showing: building a combined container with runtime and model(s) building an external volume containing unarchived models that can be mapped into the runtime container deploying a model using KServe modelmesh packaging a custom model (exported from Watson Studio) to use with the runtime","title":"Overview"},{"location":"nlp/#natural-language-processing","text":"There are a number of Natural Language Processing (NLP) services offered, covering a number of languages, within the IBM Watson Libraries for Embed. The model catalog lists all available models. In addition to the models there is a runtime which supports both gRPC and REST enpoints.","title":"Natural Language Processing"},{"location":"nlp/#deployment-options","text":"When deploying a model you need to combine the models you want with the container and there are a number of ways to do this: Create a single container with the runtime and model(s) within the container Combine the models in a single directory structure, then map that directory in to the runtime container The model containers available on the IBM Container Registry do not have a runtime installed. Their default entry point is /bin/sh -c /app/unpack_model.sh , which will expand the model into directory /app/models . This means running a model container will expand the model into directory /app/models, so if this is an external volume mounted into the container the model will be on the external volume, which could then be mounted into the NLP runtime container, which will make the model available to the runtime without having to be installed in the same container as the runtime. The NLP containers can get very large, with the runtime currently being 2.5GB> Startup times need to be considered when adding multiple models into a single container with the NLP runtime.","title":"Deployment options"},{"location":"nlp/#examples","text":"This project contains a number of examples showing: building a combined container with runtime and model(s) building an external volume containing unarchived models that can be mapped into the runtime container deploying a model using KServe modelmesh packaging a custom model (exported from Watson Studio) to use with the runtime","title":"Examples"},{"location":"setup/","text":"Setup \u00b6 Tools needed \u00b6 The examples in this repositories require the following: a container tool, such as Docker or Podman optionally a container management tool, such as Skopeo access to a Kubernetes or OpenShift cluster. Info The examples in this repo have been written using a local, bare metal OpenShift cluster, but the examples could be adapted to a Kubernetes cluster or a cluster hosted on a cloud provider Warning Make sure your Docker or Podman environment has sufficient resources. A single CPU with 2MB of memory will not be sufficient, suggest using at least 4 CPUs with 8096 MB of memory Info Using an Apple Silicon apple mac needs the container runtime to be able to run amd64 images, as the Watson Libraries are only published using amd64 architecture. Ensure that no podman machine exists then setup podman like: podman machine init --cpus 6 -m 8096 --rootful --now podman machine ssh # run the following inside the podman shell sudo -i rpm-ostree install qemu-user-static systemctl reboot Local bare metal cluster \u00b6 install OCP install Operators from Operator Hub catalog: OpenShift Data Foundation Red Hat Quay Warning There is a bug in Quay 3.7.10 that prevents large container layers from being stored, which means the Watson Libraries images cannot be stored. This article explains how to deploy a workaround Accessing container images on IBM Container Registry \u00b6 The IBM Watson Libraries for Embed container images are provided on the IBM Container Registry service, but requires an entitlement key to access them. Once you have a key you need to use it when accessing them. Local Docker / Podman / Skopeo \u00b6 When accessing the images on your local machine using Docker or Podman you need to login to the IBM container registry: Docker Podman Skopeo docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key> skopeo login cp.icr.io --username cp --password <entitlement_key> Kubernetes / OpenShift \u00b6 When accessing the images from a Kubernetes of OpenShift cluster, you need to setup a secret on the cluster then refer to it in a deployment manifest. kubectl create secret docker-registry ibm-entitlement-key --docker-server = cp.icr.io --docker-username = cp --docker-password = <entitlement key> --docker-email = <your-email> within a deployment manifest add an imagePullSecret to the template section: spec : ... template : ... spec : ... imagePullSecrets : - name : ibm-entitlement-key Setting up a local mirror \u00b6 Some of the image sizes of the IBM Watson Libraries for Embed containers are extremely large, running into multiple megabytes, so it can be advantageous to work against a local mirror. The example configuration installs Quay on the demo cluster. This can be used to hold local mirrors of the IBM Watson for Embed container images. To mirror the images the Quay mirror repository can be used to automatically mirror the images, or Skopeo can be used to copy the images from the IBM Container Registry to the local Quay registry. Using the local mirror \u00b6 The local Quay registry has a self-signed SSL certificate by default, so docker, podman, Skopeo and Kubernetes/OpenShift need to be configured to accept or ignore the self-signed X509 certificate when accessing the local registry. Docker Podman Skopeo OpenShift / Kubernetes Docker Desktop and Engine have a configuration file which allow insecure registries to be configured. You need to add the insecure-registries to the configuration file. In Docker for Desktop this can be found in the Preferences panel under the Docker Engine section. On Linux this configuration file is usually found at location /etc/docker/daemon.json . Add the section, so the file looks similar to this: { \"builder\" : { \"gc\" : { \"defaultKeepStorage\" : \"20GB\" , \"enabled\" : true } }, \"experimental\" : true , \"features\" : { \"buildkit\" : true }, \"insecure-registries\" : [ \"https://lab-registry-quay-openshift-operators.apps.ocp.lab.home\" ] } additional details of the configuration can be found in the docker documentation Podman accepts the --tls-verify=false command line argument, so if added to any command it will ignore any TLS errors. Skopeo accepts the --src-tls-verify=false and dest-tls-verify=false command line arguments, so if added to the copy command it will ignore TLS errors from the source and/or the destination registries. Enable OpenShift to pull images from local Quay container registry (using self-signed X509 certificate) by adding the self-signed certificate to the cluster configuration. This can be achieved by running the following as a cluster admin: - *Modify the REGISTRY_HOSTNAME value on the first line to match your installation* ```shell export REGISTRY_HOSTNAME=lab-registry-quay-openshift-operators.apps.ocp.lab.home export REGISTRY_PORT=443 echo \"\" | openssl s_client -showcerts -prexit -connect \"${REGISTRY_HOSTNAME}:${REGISTRY_PORT}\" 2> /dev/null | sed -n -e '/BEGIN CERTIFICATE/,/END CERTIFICATE/ p' > tmp.crt openssl x509 -in tmp.crt -text | grep Issuer oc create configmap registry-quay -n openshift-config --from-file=\"${REGISTRY_HOSTNAME}=$(pwd)/tmp.crt\" oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-quay\"}}}' --type=merge oc get image.config.openshift.io/cluster -o yaml ```","title":"Setup"},{"location":"setup/#setup","text":"","title":"Setup"},{"location":"setup/#tools-needed","text":"The examples in this repositories require the following: a container tool, such as Docker or Podman optionally a container management tool, such as Skopeo access to a Kubernetes or OpenShift cluster. Info The examples in this repo have been written using a local, bare metal OpenShift cluster, but the examples could be adapted to a Kubernetes cluster or a cluster hosted on a cloud provider Warning Make sure your Docker or Podman environment has sufficient resources. A single CPU with 2MB of memory will not be sufficient, suggest using at least 4 CPUs with 8096 MB of memory Info Using an Apple Silicon apple mac needs the container runtime to be able to run amd64 images, as the Watson Libraries are only published using amd64 architecture. Ensure that no podman machine exists then setup podman like: podman machine init --cpus 6 -m 8096 --rootful --now podman machine ssh # run the following inside the podman shell sudo -i rpm-ostree install qemu-user-static systemctl reboot","title":"Tools needed"},{"location":"setup/#local-bare-metal-cluster","text":"install OCP install Operators from Operator Hub catalog: OpenShift Data Foundation Red Hat Quay Warning There is a bug in Quay 3.7.10 that prevents large container layers from being stored, which means the Watson Libraries images cannot be stored. This article explains how to deploy a workaround","title":"Local bare metal cluster"},{"location":"setup/#accessing-container-images-on-ibm-container-registry","text":"The IBM Watson Libraries for Embed container images are provided on the IBM Container Registry service, but requires an entitlement key to access them. Once you have a key you need to use it when accessing them.","title":"Accessing container images on IBM Container Registry"},{"location":"setup/#local-docker-podman-skopeo","text":"When accessing the images on your local machine using Docker or Podman you need to login to the IBM container registry: Docker Podman Skopeo docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key> skopeo login cp.icr.io --username cp --password <entitlement_key>","title":"Local Docker / Podman / Skopeo"},{"location":"setup/#kubernetes-openshift","text":"When accessing the images from a Kubernetes of OpenShift cluster, you need to setup a secret on the cluster then refer to it in a deployment manifest. kubectl create secret docker-registry ibm-entitlement-key --docker-server = cp.icr.io --docker-username = cp --docker-password = <entitlement key> --docker-email = <your-email> within a deployment manifest add an imagePullSecret to the template section: spec : ... template : ... spec : ... imagePullSecrets : - name : ibm-entitlement-key","title":"Kubernetes / OpenShift"},{"location":"setup/#setting-up-a-local-mirror","text":"Some of the image sizes of the IBM Watson Libraries for Embed containers are extremely large, running into multiple megabytes, so it can be advantageous to work against a local mirror. The example configuration installs Quay on the demo cluster. This can be used to hold local mirrors of the IBM Watson for Embed container images. To mirror the images the Quay mirror repository can be used to automatically mirror the images, or Skopeo can be used to copy the images from the IBM Container Registry to the local Quay registry.","title":"Setting up a local mirror"},{"location":"setup/#using-the-local-mirror","text":"The local Quay registry has a self-signed SSL certificate by default, so docker, podman, Skopeo and Kubernetes/OpenShift need to be configured to accept or ignore the self-signed X509 certificate when accessing the local registry. Docker Podman Skopeo OpenShift / Kubernetes Docker Desktop and Engine have a configuration file which allow insecure registries to be configured. You need to add the insecure-registries to the configuration file. In Docker for Desktop this can be found in the Preferences panel under the Docker Engine section. On Linux this configuration file is usually found at location /etc/docker/daemon.json . Add the section, so the file looks similar to this: { \"builder\" : { \"gc\" : { \"defaultKeepStorage\" : \"20GB\" , \"enabled\" : true } }, \"experimental\" : true , \"features\" : { \"buildkit\" : true }, \"insecure-registries\" : [ \"https://lab-registry-quay-openshift-operators.apps.ocp.lab.home\" ] } additional details of the configuration can be found in the docker documentation Podman accepts the --tls-verify=false command line argument, so if added to any command it will ignore any TLS errors. Skopeo accepts the --src-tls-verify=false and dest-tls-verify=false command line arguments, so if added to the copy command it will ignore TLS errors from the source and/or the destination registries. Enable OpenShift to pull images from local Quay container registry (using self-signed X509 certificate) by adding the self-signed certificate to the cluster configuration. This can be achieved by running the following as a cluster admin: - *Modify the REGISTRY_HOSTNAME value on the first line to match your installation* ```shell export REGISTRY_HOSTNAME=lab-registry-quay-openshift-operators.apps.ocp.lab.home export REGISTRY_PORT=443 echo \"\" | openssl s_client -showcerts -prexit -connect \"${REGISTRY_HOSTNAME}:${REGISTRY_PORT}\" 2> /dev/null | sed -n -e '/BEGIN CERTIFICATE/,/END CERTIFICATE/ p' > tmp.crt openssl x509 -in tmp.crt -text | grep Issuer oc create configmap registry-quay -n openshift-config --from-file=\"${REGISTRY_HOSTNAME}=$(pwd)/tmp.crt\" oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-quay\"}}}' --type=merge oc get image.config.openshift.io/cluster -o yaml ```","title":"Using the local mirror"},{"location":"speech/","text":"Speech services \u00b6 There are 2 speech services offered by IBM Watson Libraries for Embed: Text to Speech Speech to Text There are examples of both provided in these examples. The speech libraries contain a number of different containers: the core runtime for speech to text and text to speech a catalog image for speech to text and text to speech a set of 8 kHz (Telephony) and 16 kHz (Multimedia) sampling rate models for multiple libraries To create a run time the core runtime, the catalog image and a set of language images need to be combined. There are 2 options available for this: build a single container containing all the content required - local example use pod init containers to generate the combined run time as part of a deployment on Kubernetes - OpenShift example Configuration \u00b6 When combining multiple container images you need to provide some configuration files. Some of the files control the creation of the combined runtime, while others are used to configure the runtime. When creating a local runtime the configuration is created in a number of files but in the Kubernetes deployment the configuration is created in a ConfigMap. Both of these approaches contain the same content. The configuration is made up of the following files: env_config.json resourceRequirements.py sessionPools.py sessionPools.yaml The contents of these files is detailed in the documentation . Adding a model to the configuration files \u00b6 The functionality of the deployed runtime is controlled by the models included in the runtime. If you want to be able to handle a specific language at a certain sample rate (Telephony vs Multimedia) or vocalise with a specific voice you need to have the appropriate model built into the runtime. It is important to ensure that all models that you want to include in the runtime are detailed in the configuration files for the local deployment and the Kubernetes ConfigMap manifest in config.yaml for a Kubernetes deployment. Find the additional model images from the text to speech model catalog or the speech to text model catalog Update the clusterGroups.default.models array in env_config.json Update the sessionPoolPolicies.PreWarmingPolicy array in sessionPools.yaml . You also have to bring in the model container in the Containerfile for local running or as an additional init container in the Deployment on Kubernetes as detailed in the next 2 sections. Adding a model to the Containerfile \u00b6 When adding a new model you need to: Add the model image to the top of the Containerfile in a new FROM <container registry>/<model-image>:<model image tag> as <short-model-image-name> statement Populate the intermediate model cache by adding another COPY --chown=watson:0 --from=<short-model-image-name>/* /models/pool2/ to the Containerfile Adding a model to the Kubernetes Deployment manifest \u00b6 When adding a new model you need to: Add a new init container to the Deployment manifest in the spec.template.spec.initContainers array: - name : <model-image> image : <container registry>/<model-image>:<model image tag> args : - sh - -c - cp model/* /models/pool2 env : - name : ACCEPT_LICENSE value : \"true\" resources : limits : cpu : 1 ephemeral-storage : 1Gi memory : 1Gi requests : cpu : 100m ephemeral-storage : 1Gi memory : 256Mi volumeMounts : - name : models mountPath : /models/pool2 Secure communication \u00b6 The speech runtime containers don't support TLS termination, only http. The examples use OpenShift Routes to enforce TLS termination, so all traffic coming into the cluster will be protected by TLS, but the traffic from the cluster ingress to the speech runtime will be unencrypted, which isn't ideal. To solve this the watson-stt-haproxy and watson-tts-haproxy containers can be run in the same pod as the appropriate speech runtime. The service and route can then be modified to sent traffic to the haproxy container, which will then use a localhost address (stays within the pod) to send the traffic from the proxy into the speech runtime. Todo Add the watson-stt-haproxy and watson-tts-haproxy to the example deployment, service and route manifest files to enable internet -> pod -> speech runtime fully encrypted traffic Metering \u00b6 You need to configure your production runtime to store logs on persistent storage and enable metering in the configuration so you have an accurate record of the usage as detailed in the speech to text documentation and text to speech documentation Todo Add a suitable metering configuration for persisting and processing logs to the example deployments and cluster setup","title":"Overview"},{"location":"speech/#speech-services","text":"There are 2 speech services offered by IBM Watson Libraries for Embed: Text to Speech Speech to Text There are examples of both provided in these examples. The speech libraries contain a number of different containers: the core runtime for speech to text and text to speech a catalog image for speech to text and text to speech a set of 8 kHz (Telephony) and 16 kHz (Multimedia) sampling rate models for multiple libraries To create a run time the core runtime, the catalog image and a set of language images need to be combined. There are 2 options available for this: build a single container containing all the content required - local example use pod init containers to generate the combined run time as part of a deployment on Kubernetes - OpenShift example","title":"Speech services"},{"location":"speech/#configuration","text":"When combining multiple container images you need to provide some configuration files. Some of the files control the creation of the combined runtime, while others are used to configure the runtime. When creating a local runtime the configuration is created in a number of files but in the Kubernetes deployment the configuration is created in a ConfigMap. Both of these approaches contain the same content. The configuration is made up of the following files: env_config.json resourceRequirements.py sessionPools.py sessionPools.yaml The contents of these files is detailed in the documentation .","title":"Configuration"},{"location":"speech/#adding-a-model-to-the-configuration-files","text":"The functionality of the deployed runtime is controlled by the models included in the runtime. If you want to be able to handle a specific language at a certain sample rate (Telephony vs Multimedia) or vocalise with a specific voice you need to have the appropriate model built into the runtime. It is important to ensure that all models that you want to include in the runtime are detailed in the configuration files for the local deployment and the Kubernetes ConfigMap manifest in config.yaml for a Kubernetes deployment. Find the additional model images from the text to speech model catalog or the speech to text model catalog Update the clusterGroups.default.models array in env_config.json Update the sessionPoolPolicies.PreWarmingPolicy array in sessionPools.yaml . You also have to bring in the model container in the Containerfile for local running or as an additional init container in the Deployment on Kubernetes as detailed in the next 2 sections.","title":"Adding a model to the configuration files"},{"location":"speech/#adding-a-model-to-the-containerfile","text":"When adding a new model you need to: Add the model image to the top of the Containerfile in a new FROM <container registry>/<model-image>:<model image tag> as <short-model-image-name> statement Populate the intermediate model cache by adding another COPY --chown=watson:0 --from=<short-model-image-name>/* /models/pool2/ to the Containerfile","title":"Adding a model to the Containerfile"},{"location":"speech/#adding-a-model-to-the-kubernetes-deployment-manifest","text":"When adding a new model you need to: Add a new init container to the Deployment manifest in the spec.template.spec.initContainers array: - name : <model-image> image : <container registry>/<model-image>:<model image tag> args : - sh - -c - cp model/* /models/pool2 env : - name : ACCEPT_LICENSE value : \"true\" resources : limits : cpu : 1 ephemeral-storage : 1Gi memory : 1Gi requests : cpu : 100m ephemeral-storage : 1Gi memory : 256Mi volumeMounts : - name : models mountPath : /models/pool2","title":"Adding a model to the Kubernetes Deployment manifest"},{"location":"speech/#secure-communication","text":"The speech runtime containers don't support TLS termination, only http. The examples use OpenShift Routes to enforce TLS termination, so all traffic coming into the cluster will be protected by TLS, but the traffic from the cluster ingress to the speech runtime will be unencrypted, which isn't ideal. To solve this the watson-stt-haproxy and watson-tts-haproxy containers can be run in the same pod as the appropriate speech runtime. The service and route can then be modified to sent traffic to the haproxy container, which will then use a localhost address (stays within the pod) to send the traffic from the proxy into the speech runtime. Todo Add the watson-stt-haproxy and watson-tts-haproxy to the example deployment, service and route manifest files to enable internet -> pod -> speech runtime fully encrypted traffic","title":"Secure communication"},{"location":"speech/#metering","text":"You need to configure your production runtime to store logs on persistent storage and enable metering in the configuration so you have an accurate record of the usage as detailed in the speech to text documentation and text to speech documentation Todo Add a suitable metering configuration for persisting and processing logs to the example deployments and cluster setup","title":"Metering"},{"location":"stt-kube/","text":"Speech to Text Kubernetes \u00b6 This page covers how to get the Speech to Text service running on OpenShift. The files relating to this example can be found in the repository here Deploy to OpenShift \u00b6 Navigate to where the files from this repo have been cloned to your local machine and change to the speech/stt_kube directory Create a new project on the cluster oc new-project demo-stt Apply all the manifest files to your cluster oc apply -f <directory containing manifest files> Testing the deployment \u00b6 To verify the service is running correctly the curl utility can be used to make requests of the service. You need a test audio file. One is provided in the stt_kube directory named test.wav . This was created on a MacOS system using the standard Voice Memos app. Once a recording was made, select it in the All Recordings list, copy it (\u2318C) then paste it to the Desktop. Use the Convertio site, or similar, to convert the m4a file to a wav file. Todo Work out process to create audio file on Linux and Windows systems To test the speech to text container the curl utility can be used to submit requests, alter the URL to the route of the service on your cluster: curl -k \"https://stt-embed-demo-stt.apps.ocp.lab.home/speech-to-text/api/v1/recognize\" --header \"Content-Type: audio/wav\" --data-binary @test.wav","title":"Kubernetes deployment"},{"location":"stt-kube/#speech-to-text-kubernetes","text":"This page covers how to get the Speech to Text service running on OpenShift. The files relating to this example can be found in the repository here","title":"Speech to Text Kubernetes"},{"location":"stt-kube/#deploy-to-openshift","text":"Navigate to where the files from this repo have been cloned to your local machine and change to the speech/stt_kube directory Create a new project on the cluster oc new-project demo-stt Apply all the manifest files to your cluster oc apply -f <directory containing manifest files>","title":"Deploy to OpenShift"},{"location":"stt-kube/#testing-the-deployment","text":"To verify the service is running correctly the curl utility can be used to make requests of the service. You need a test audio file. One is provided in the stt_kube directory named test.wav . This was created on a MacOS system using the standard Voice Memos app. Once a recording was made, select it in the All Recordings list, copy it (\u2318C) then paste it to the Desktop. Use the Convertio site, or similar, to convert the m4a file to a wav file. Todo Work out process to create audio file on Linux and Windows systems To test the speech to text container the curl utility can be used to submit requests, alter the URL to the route of the service on your cluster: curl -k \"https://stt-embed-demo-stt.apps.ocp.lab.home/speech-to-text/api/v1/recognize\" --header \"Content-Type: audio/wav\" --data-binary @test.wav","title":"Testing the deployment"},{"location":"stt-local/","text":"Speech to Text Local \u00b6 This page describes how to run Speech to Text on your local system using Docker or Podman. The product documentation will provide more details and additional capabilities not shown in this example. Access to images \u00b6 To run this example you need to have access to the IBM Watson Libraries for Embed container images. These are available on the IBM Container Registry, but you need an entitlement key For the code in this repo I use a local mirror, hosted in a Project Quay repository. If you are running against the IBM registry then you will need to change the images from lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs to cp.icr.io/cp/ai and login to the repository before trying to access any images with Docker Podman docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key> Building the runtime \u00b6 To create a usable speech to text runtime container, the standard runtime needs to be combined with appropriate model(s). There are a number of standard models provided. In addition to the model(s) a configuration needs to be provided. This configuration needs to be customised to match the included model(s). The files used in this example can be found here . To build the container image run the command: Docker Podman docker build -f Containerfile . -t stt-standalone podman build --tls-verify = false -f Containerfile . -t stt-standalone You should store the container image in a repository if you want to use it on a Kubernetes platform or make it available to other developers to run locally Docker Podman docker tag stt-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 docker push lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 podman tag stt-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 podman push --tls-verify = false lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 Running the container \u00b6 The container can be run on your local system. The following commands assume you have the tool on your local system (either built or pulled from a repository) Docker Podman docker run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 stt-standalone podman run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 stt-standalone Submitting requests to the container \u00b6 To submit a request to the container you need an audio file. There is one in the repository, test.wav. This was created on a MacOS system using the standard Voice Memos app. Once a recording was made, select it in the All Recordings list, copy it (\u2318C) then paste it to the Desktop. Use the Convertio site, or similar, to convert the m4a file to a wav file. Todo Work out process to create audio file on Linux and Windows systems To test the speech to text container the curl utility can be used to submit requests: curl \"http://localhost:1080/speech-to-text/api/v1/recognize\" --header \"Content-Type: audio/wav\" --data-binary @test.wav Refer to the API reference for details of the requests that can be made","title":"Local use"},{"location":"stt-local/#speech-to-text-local","text":"This page describes how to run Speech to Text on your local system using Docker or Podman. The product documentation will provide more details and additional capabilities not shown in this example.","title":"Speech to Text Local"},{"location":"stt-local/#access-to-images","text":"To run this example you need to have access to the IBM Watson Libraries for Embed container images. These are available on the IBM Container Registry, but you need an entitlement key For the code in this repo I use a local mirror, hosted in a Project Quay repository. If you are running against the IBM registry then you will need to change the images from lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs to cp.icr.io/cp/ai and login to the repository before trying to access any images with Docker Podman docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key>","title":"Access to images"},{"location":"stt-local/#building-the-runtime","text":"To create a usable speech to text runtime container, the standard runtime needs to be combined with appropriate model(s). There are a number of standard models provided. In addition to the model(s) a configuration needs to be provided. This configuration needs to be customised to match the included model(s). The files used in this example can be found here . To build the container image run the command: Docker Podman docker build -f Containerfile . -t stt-standalone podman build --tls-verify = false -f Containerfile . -t stt-standalone You should store the container image in a repository if you want to use it on a Kubernetes platform or make it available to other developers to run locally Docker Podman docker tag stt-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 docker push lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 podman tag stt-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1 podman push --tls-verify = false lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/stt-standalone:0.0.1","title":"Building the runtime"},{"location":"stt-local/#running-the-container","text":"The container can be run on your local system. The following commands assume you have the tool on your local system (either built or pulled from a repository) Docker Podman docker run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 stt-standalone podman run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 stt-standalone","title":"Running the container"},{"location":"stt-local/#submitting-requests-to-the-container","text":"To submit a request to the container you need an audio file. There is one in the repository, test.wav. This was created on a MacOS system using the standard Voice Memos app. Once a recording was made, select it in the All Recordings list, copy it (\u2318C) then paste it to the Desktop. Use the Convertio site, or similar, to convert the m4a file to a wav file. Todo Work out process to create audio file on Linux and Windows systems To test the speech to text container the curl utility can be used to submit requests: curl \"http://localhost:1080/speech-to-text/api/v1/recognize\" --header \"Content-Type: audio/wav\" --data-binary @test.wav Refer to the API reference for details of the requests that can be made","title":"Submitting requests to the container"},{"location":"tts-kube/","text":"Text to Speech Kubernetes \u00b6 This page covers how to get the Text to Speech service running on OpenShift. The files relating to this example can be found in the repository here Deploy to OpenShift \u00b6 Navigate to where the files from this repo have been cloned to your local machine and change to the speech/tts_kube directory Create a new project on the cluster oc new-project demo-tts Apply all the manifest files to your cluster oc apply -f <directory containing manifest files> Testing the deployment \u00b6 To test the speech to text container the curl utility can be used to submit requests, alter the URL to the route of the service on your cluster: curl -k \"https://tts-embed-demo-tts.apps.ocp.lab.home/text-to-speech/api/v1/synthesize\" \\ --header \"Content-Type: application/json\" --data '{\"text\":\"Hello world\"}' \\ --header \"Accept: audio/wav\" --output output.wav This used the default voice set in the configuration. You can also specify a different voice as part of the request, so long as the voice model was built into the runtime: curl -k \"https://tts-embed-demo-tts.apps.ocp.lab.home/text-to-speech/api/v1/synthesize?voice=en-GB_JamesV3Voice\" \\ --header \"Content-Type: application/json\" \\ --data '{\"text\":\"Hello! Isn''t it a wonderful day.\"}' \\ --header \"Accept: audio/wav\" \\ --output british-test.wav Refer to the API reference for details of the requests that can be made","title":"Kubernetes deployment"},{"location":"tts-kube/#text-to-speech-kubernetes","text":"This page covers how to get the Text to Speech service running on OpenShift. The files relating to this example can be found in the repository here","title":"Text to Speech Kubernetes"},{"location":"tts-kube/#deploy-to-openshift","text":"Navigate to where the files from this repo have been cloned to your local machine and change to the speech/tts_kube directory Create a new project on the cluster oc new-project demo-tts Apply all the manifest files to your cluster oc apply -f <directory containing manifest files>","title":"Deploy to OpenShift"},{"location":"tts-kube/#testing-the-deployment","text":"To test the speech to text container the curl utility can be used to submit requests, alter the URL to the route of the service on your cluster: curl -k \"https://tts-embed-demo-tts.apps.ocp.lab.home/text-to-speech/api/v1/synthesize\" \\ --header \"Content-Type: application/json\" --data '{\"text\":\"Hello world\"}' \\ --header \"Accept: audio/wav\" --output output.wav This used the default voice set in the configuration. You can also specify a different voice as part of the request, so long as the voice model was built into the runtime: curl -k \"https://tts-embed-demo-tts.apps.ocp.lab.home/text-to-speech/api/v1/synthesize?voice=en-GB_JamesV3Voice\" \\ --header \"Content-Type: application/json\" \\ --data '{\"text\":\"Hello! Isn''t it a wonderful day.\"}' \\ --header \"Accept: audio/wav\" \\ --output british-test.wav Refer to the API reference for details of the requests that can be made","title":"Testing the deployment"},{"location":"tts-local/","text":"Text to Speech Local This page describes how to run Text to Speech on your local system using Docker or Podman. The product documentation will provide more details and additional capabilities not shown in this example. Access to images \u00b6 To run this example you need to have access to the IBM Watson Libraries for Embed container images. These are available on the IBM Container Registry, but you need an entitlement key For the code in this repo I use a local mirror, hosted in a Project Quay repository. If you are running against the IBM registry then you will need to change the images from lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs to cp.icr.io/cp/ai and login to the repository before trying to access any images with Docker Podman docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key> Building the runtime \u00b6 To create a usable text to speech runtime container, the standard runtime needs to be combined with appropriate model(s). There are a number of standard models provided. In addition to the model(s) a configuration needs to be provided. This configuration needs to be customised to match the included model(s). The files used in this example can be found here . To build the container image run the command: Docker Podman docker build -f Containerfile . -t tts-standalone podman build --tls-verify = false -f Containerfile . -t tts-standalone You should store the container image in a repository if you want to use it on a Kubernetes platform or make it available to other developers to run locally Docker Podman docker tag tts-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 docker push lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 podman tag tts-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 podman push --tls-verify = false lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 Running the container locally \u00b6 The container can be run on your local system. The following commands assume you have the tool on your local system (either built or pulled from a repository) Docker Podman docker run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 nlp-standalone podman run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 nlp-standalone Submitting requests to the container \u00b6 To test the text to speech container the curl utility can be used to submit requests: curl \"http://localhost:1080/text-to-speech/api/v1/synthesize\" \\ --header \"Content-Type: application/json\" --data '{\"text\":\"Hello world\"}' \\ --header \"Accept: audio/wav\" --output output.wav This used the default voice set in the configuration. You can also specify a different voice as part of the request, so long as the voice model was built into the runtime: curl \"http://localhost:1080/text-to-speech/api/v1/synthesize?voice=en-GB_JamesV3Voice\" \\ --header \"Content-Type: application/json\" \\ --data '{\"text\":\"Hello! Isn''t it a wonderful day.\"}' \\ --header \"Accept: audio/wav\" \\ --output british-test.wav Refer to the API reference for details of the requests that can be made","title":"Local use"},{"location":"tts-local/#access-to-images","text":"To run this example you need to have access to the IBM Watson Libraries for Embed container images. These are available on the IBM Container Registry, but you need an entitlement key For the code in this repo I use a local mirror, hosted in a Project Quay repository. If you are running against the IBM registry then you will need to change the images from lab-registry-quay-openshift-operators.apps.ocp.lab.home/watson-libs to cp.icr.io/cp/ai and login to the repository before trying to access any images with Docker Podman docker login cp.icr.io --username cp --password <entitlement_key> podman login cp.icr.io --username cp --password <entitlement_key>","title":"Access to images"},{"location":"tts-local/#building-the-runtime","text":"To create a usable text to speech runtime container, the standard runtime needs to be combined with appropriate model(s). There are a number of standard models provided. In addition to the model(s) a configuration needs to be provided. This configuration needs to be customised to match the included model(s). The files used in this example can be found here . To build the container image run the command: Docker Podman docker build -f Containerfile . -t tts-standalone podman build --tls-verify = false -f Containerfile . -t tts-standalone You should store the container image in a repository if you want to use it on a Kubernetes platform or make it available to other developers to run locally Docker Podman docker tag tts-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 docker push lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 podman tag tts-standalone lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1 podman push --tls-verify = false lab-registry-quay-openshift-operators.apps.ocp.lab.home/brian/tts-standalone:0.0.1","title":"Building the runtime"},{"location":"tts-local/#running-the-container-locally","text":"The container can be run on your local system. The following commands assume you have the tool on your local system (either built or pulled from a repository) Docker Podman docker run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 nlp-standalone podman run --rm -it --env ACCEPT_LICENSE = true --publish 1080 :1080 nlp-standalone","title":"Running the container locally"},{"location":"tts-local/#submitting-requests-to-the-container","text":"To test the text to speech container the curl utility can be used to submit requests: curl \"http://localhost:1080/text-to-speech/api/v1/synthesize\" \\ --header \"Content-Type: application/json\" --data '{\"text\":\"Hello world\"}' \\ --header \"Accept: audio/wav\" --output output.wav This used the default voice set in the configuration. You can also specify a different voice as part of the request, so long as the voice model was built into the runtime: curl \"http://localhost:1080/text-to-speech/api/v1/synthesize?voice=en-GB_JamesV3Voice\" \\ --header \"Content-Type: application/json\" \\ --data '{\"text\":\"Hello! Isn''t it a wonderful day.\"}' \\ --header \"Accept: audio/wav\" \\ --output british-test.wav Refer to the API reference for details of the requests that can be made","title":"Submitting requests to the container"}]}